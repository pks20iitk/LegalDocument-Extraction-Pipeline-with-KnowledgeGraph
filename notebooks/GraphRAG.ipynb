{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "105a2123",
   "metadata": {},
   "source": [
    "# Text2Cypher, Graph Retrieval Arguments Generation(RAG), Graph RAG and Graph+Vector RAG\n",
    "\n",
    "To enable Cognitive intelligence App towards private data, RAG + LLM and Knowledge Graph are state-of-the-art approaches.\n",
    "\n",
    "In this demo, we will explain know-how of four types of approaches and compare the trade-off and performance among them.\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n",
    "\n",
    "## Background, RAG\n",
    "\n",
    "Below digrams are showing how RAG works:\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐ ┌────┐               │             \n",
    "               │ 3  │ │ 96 │                             \n",
    "             │ └────┘ └────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "In VectorDB based RAG, we create embeddings of each node(chunk), and find TopK related ones towards a given question during the query. In the above diagram, nodes `3` and `96` were fetched as the TopK related nodes, used to help answer the user query. \n",
    "\n",
    "## Background Graph RAG\n",
    "\n",
    "In Graph RAG, we will extract relationships between entities, representing concise facts from each node. It would look something like this:\n",
    "\n",
    "```\n",
    "Node Split and Embedding\n",
    "\n",
    "┌────┬────┬────┬────┐\n",
    "│ 1  │ 2  │ 3  │ 4  │\n",
    "├────┴────┴────┴────┤\n",
    "│  Docs/Knowledge   │\n",
    "│        ...        │\n",
    "├────┬────┬────┬────┤\n",
    "│ 95 │ 96 │    │    │\n",
    "└────┴────┴────┴────┘\n",
    "```\n",
    "\n",
    "Then, if we zoomed in of it:\n",
    "\n",
    "```\n",
    "       Node Split and Embedding, with Knowledge Graph being extracted\n",
    "\n",
    "┌──────────────────┬──────────────────┬──────────────────┬──────────────────┐\n",
    "│ .─.       .─.    │  .─.       .─.   │            .─.   │  .─.       .─.   │\n",
    "│( x )─────▶ y )   │ ( x )─────▶ a )  │           ( j )  │ ( m )◀────( x )  │\n",
    "│ `▲'       `─'    │  `─'       `─'   │            `─'   │  `─'       `─'   │\n",
    "│  │     1         │        2         │        3    │    │        4         │\n",
    "│ .─.              │                  │            .▼.   │                  │\n",
    "│( z )─────────────┼──────────────────┼──────────▶( i )─┐│                  │\n",
    "│ `◀────┐          │                  │            `─'  ││                  │\n",
    "├───────┼──────────┴──────────────────┴─────────────────┼┴──────────────────┤\n",
    "│       │                      Docs/Knowledge           │                   │\n",
    "│       │                            ...                │                   │\n",
    "│       │                                               │                   │\n",
    "├───────┼──────────┬──────────────────┬─────────────────┼┬──────────────────┤\n",
    "│  .─.  └──────.   │  .─.             │                 ││  .─.             │\n",
    "│ ( x ◀─────( b )  │ ( x )            │                 └┼▶( n )            │\n",
    "│  `─'       `─'   │  `─'             │                  │  `─'             │\n",
    "│        95   │    │   │    96        │                  │   │    98        │\n",
    "│            .▼.   │  .▼.             │                  │   ▼              │\n",
    "│           ( c )  │ ( d )            │                  │  .─.             │\n",
    "│            `─'   │  `─'             │                  │ ( x )            │\n",
    "└──────────────────┴──────────────────┴──────────────────┴──`─'─────────────┘\n",
    "```\n",
    "\n",
    "Where, knowledge, the more granular spliting and information with higher density, optionally multi-hop of `x -> y`, `i -> j -> z -> x` etc... across many more nodes(chunks) than K(in TopK search) could be inlucded in Retrievers. And we believe there are cases that this additional work matters.\n",
    "\n",
    "But how/how well exactly does it work? Let's see in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75004d1",
   "metadata": {},
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## 1.1 Prepare for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3282e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_base = os.getenv(\"OPENAI_BASE\")\n",
    "openai_deployment = os.getenv(\"AZURE_OPEN_AI_MODEL\")\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://prod-open-ai-service.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"30c82b416ff4456faa26b1644a83ab4b\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5363496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Azure OpenAI\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index import LangchainEmbedding\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    KnowledgeGraphIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index import set_global_service_context\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.INFO\n",
    ")  # logging.DEBUG for more verbose output\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://prod-open-ai-service.openai.azure.com/\"\n",
    "openai.api_version = \"2023-05-15\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"30c82b416ff4456faa26b1644a83ab4b\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"PROD-GPT-16K-TURBO\",\n",
    "    temperature=0,\n",
    "    openai_api_version=openai.api_version,\n",
    "    model_kwargs={\n",
    "        \"api_key\": openai.api_key,\n",
    "        \"api_base\": openai.api_base,\n",
    "        \"api_type\": openai.api_type,\n",
    "        \"api_version\": openai.api_version,\n",
    "    },\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embedding_llm = LangchainEmbedding(\n",
    "    OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=\"PROD-text-embedding-ada-002\",\n",
    "        openai_api_key=openai.api_key,\n",
    "        openai_api_base=openai.api_base,\n",
    "        openai_api_type=openai.api_type,\n",
    "        openai_api_version=openai.api_version,\n",
    "    ),\n",
    "    embed_batch_size=16,\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embedding_llm,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b17442e",
   "metadata": {},
   "source": [
    "## 1.2. Prepare for NebulaGraph as Graph Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ca9fa",
   "metadata": {},
   "source": [
    "❗Access NebulaGraph Console to **create space** and **graph schema**\n",
    "\n",
    "```sql\n",
    "CREATE SPACE guardians(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    ":sleep 10;\n",
    "USE guardians;\n",
    "CREATE TAG entity(name string);\n",
    "CREATE EDGE relationship(relationship string);\n",
    ":sleep 10;\n",
    "CREATE TAG INDEX entity_index ON entity(name(256));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6cf0e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nebula3-python in d:\\pragyaa\\.venv\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: ipython-ngql in d:\\pragyaa\\.venv\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: httplib2>=0.20.0 in d:\\pragyaa\\.venv\\lib\\site-packages (from nebula3-python) (0.22.0)\n",
      "Requirement already satisfied: future>=0.18.0 in d:\\pragyaa\\.venv\\lib\\site-packages (from nebula3-python) (0.18.3)\n",
      "Requirement already satisfied: six>=1.16.0 in d:\\pragyaa\\.venv\\lib\\site-packages (from nebula3-python) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2021.1 in d:\\pragyaa\\.venv\\lib\\site-packages (from nebula3-python) (2023.3)\n",
      "Requirement already satisfied: Jinja2 in d:\\pragyaa\\.venv\\lib\\site-packages (from ipython-ngql) (3.1.2)\n",
      "Requirement already satisfied: pandas in d:\\pragyaa\\.venv\\lib\\site-packages (from ipython-ngql) (2.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in d:\\pragyaa\\.venv\\lib\\site-packages (from httplib2>=0.20.0->nebula3-python) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\pragyaa\\.venv\\lib\\site-packages (from Jinja2->ipython-ngql) (2.1.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in d:\\pragyaa\\.venv\\lib\\site-packages (from pandas->ipython-ngql) (1.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\pragyaa\\.venv\\lib\\site-packages (from pandas->ipython-ngql) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\pragyaa\\.venv\\lib\\site-packages (from pandas->ipython-ngql) (2023.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nebula3-python ipython-ngql\n",
    "\n",
    "os.environ['NEBULA_USER'] = \"root\"\n",
    "os.environ['NEBULA_PASSWORD'] = \"nebula\" # default password\n",
    "os.environ['NEBULA_ADDRESS'] = \"127.0.0.1:9669\" # assumed we have NebulaGraph installed locally\n",
    "\n",
    "space_name = \"legalgraph\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"] # default, could be omit if create from an empty kg\n",
    "tags = [\"entity\"] # default, could be omit if create from an empty kg\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5613a",
   "metadata": {},
   "source": [
    "## 2. Build the Knowledge Graph\n",
    "\n",
    "In our demo, the Knowledge Graph was created with LLM.\n",
    "\n",
    "We simply do so leveragint the `KnowledgeGraphIndex` from LlamaIndex, when creating it, Triplets will be extracted with LLM and evantually persisted into `NebulaGraphStore`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb7083",
   "metadata": {},
   "source": [
    "### 2.1 Preprocess Data\n",
    "\n",
    "We will download and preprecess data from:\n",
    "    https://en.wikipedia.org/wiki/Guardians_of_the_Galaxy_Vol._3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "\n",
    "documents = loader.load_data(pages=['Guardians of the Galaxy Vol. 3'], auto_suggest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbae6a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Pool Created\n",
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n",
      "Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR]:\n",
      " 'IPythonNGQL' object has no attribute '_decode_value'\n",
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n",
      "Get connection to ('127.0.0.1', 9669)\n",
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n",
      "Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext ngql\n",
    "connection_string = f\"--address localhost --port 9669 --user root --password nebula\"\n",
    "%ngql {connection_string}\n",
    "%ngql CREATE SPACE IF NOT EXISTS legalgraph(vid_type=FIXED_STRING(256), partition_num=1, replica_factor=1);\n",
    "%ngql USE legalgraph;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62bcd5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n",
      "Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql\n",
    "CREATE TAG IF NOT EXISTS entity(name string);\n",
    "CREATE EDGE IF NOT EXISTS relationship(relationship string);\n",
    "CREATE TAG INDEX IF NOT EXISTS entity_index ON entity(name(256));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e1ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NEBULA_USER\"] = \"root\"\n",
    "os.environ[\"NEBULA_PASSWORD\"] = \"nebula\"  # default is \"nebula\"\n",
    "os.environ[\n",
    "    \"NEBULA_ADDRESS\"\n",
    "] = \"127.0.0.1:9669\"  # assumed we have NebulaGraph installed locally\n",
    "\n",
    "space_name = \"legalgraph\"\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\n",
    "    \"relationship\"\n",
    "]  # default, could be omit if create from an empty kg\n",
    "tags = [\"entity\"]  # default, could be omit if create from an empty kg\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6569544",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data-legal\"\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "docs = SimpleDirectoryReader(DATA_PATH).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21c03b",
   "metadata": {},
   "source": [
    "### 2.2 Extract Triplets and Save to NebulaGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f3668",
   "metadata": {},
   "source": [
    "This call will take some time, it'll extract entities and relationships and store them into NebulaGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c05437d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    docs,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3efdc4",
   "metadata": {},
   "source": [
    "## 3 Create VectorStoreIndex for RAG\n",
    "\n",
    "To compare with/work together with VectorDB based RAG, let's also create a `VectorStoreIndex`.\n",
    "\n",
    "During the creation, same data source will be split into chunks and embedding of them will be created, during the RAG query time, the top-k related embeddings will be vector-searched with the embedding of the question.\n",
    "\n",
    "```\n",
    "                  RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐ ┌────┐               │             \n",
    "               │ 3  │ │ 96 │                             \n",
    "             │ └────┘ └────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "In Llama Index, this could be done with oneline of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "474c2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c2aee",
   "metadata": {},
   "source": [
    "## 4. Persist and Load from disk Llama Indexes(Optional)\n",
    "\n",
    "Both the `KnowledgeGraphIndex` and `VectorStoreIndex` will be created only once, afterwards, we could persist their in-memory context to enable their reuse from disk anytime.\n",
    "\n",
    "#### Persist\n",
    "\n",
    "```python\n",
    "# persist KG Index(Only MetaData will be persisted, KG is in NebulaGraph)\n",
    "kg_index.storage_context.persist(persist_dir='./storage_graph')\n",
    "\n",
    "# persist Vector Index\n",
    "vector_index.storage_context.persist(persist_dir='./storage_vector')\n",
    "\n",
    "```\n",
    "\n",
    "Then the files are created:\n",
    "\n",
    "```bash\n",
    "$ ls -l ./storage_*\n",
    "\n",
    "storage_graph:\n",
    "total 6384\n",
    "-rw-r--r--@ 1 weyl  staff    44008 Jul 14 11:06 docstore.json\n",
    "-rw-r--r--@ 1 weyl  staff  3219385 Jul 14 11:06 index_store.json\n",
    "-rw-r--r--@ 1 weyl  staff       51 Jul 14 11:06 vector_store.json\n",
    "\n",
    "storage_vector:\n",
    "total 712\n",
    "-rw-r--r--@ 1 weyl  staff   44008 Jul 14 11:06 docstore.json\n",
    "-rw-r--r--@ 1 weyl  staff      18 Jul 14 11:06 graph_store.json\n",
    "-rw-r--r--@ 1 weyl  staff    1003 Jul 14 11:06 index_store.json\n",
    "-rw-r--r--@ 1 weyl  staff  311028 Jul 14 11:06 vector_store.json\n",
    "```\n",
    "\n",
    "#### Restore\n",
    "\n",
    "So we could restore the index from disk like:\n",
    "\n",
    "```python\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir='./storage_graph', graph_store=graph_store)\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "storage_context_vector = StorageContext.from_defaults(persist_dir='./storage_vector')\n",
    "vector_index = load_index_from_storage(\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context_vector\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd3f4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "vector_index.storage_context.persist(persist_dir='./storage_vector')\n",
    "kg_index.storage_context.persist(persist_dir='./storage_graph')\n",
    "\n",
    "\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir='./storage_graph', graph_store=graph_store)\n",
    "kg_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True,\n",
    ")\n",
    "\n",
    "storage_context_vector = StorageContext.from_defaults(persist_dir='./storage_vector')\n",
    "vector_index = load_index_from_storage(\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2eb936",
   "metadata": {},
   "source": [
    "## 5. Prepare for different query approaches\n",
    "\n",
    "We will do 4 types of query approaches with LLM, KG, VectorDB:\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5524b2e",
   "metadata": {},
   "source": [
    "### 5.1 text-to-NebulaGraphCypher\n",
    "\n",
    "Text-to-NebulaGraphCypher approach Translate task/question into a Graph Cypher Query, and answer based on its query result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "097b25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import KnowledgeGraphQueryEngine\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import NebulaGraphStore\n",
    "\n",
    "nl2kg_query_engine = KnowledgeGraphQueryEngine(\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c590de2",
   "metadata": {},
   "source": [
    "### 5.2 Graph RAG query engine\n",
    "\n",
    "Graph RAG takes SubGraphs related to entities of the task/question as Context.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me about x, please │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ Below are knowledge about x │             \n",
    "               x->y<-z,x->h->i, m<-n,...                            \n",
    "             │ Please answer based on them │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b01e372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_rag_query_engine = kg_index.as_query_engine(\n",
    "    include_text=False,\n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06682474",
   "metadata": {},
   "source": [
    "### 5.3 Vector RAG query engine\n",
    "\n",
    "Vector RAG is the common approach to find topK semantic related doc chunks as context to synthesize the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37713b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_rag_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93396d7",
   "metadata": {},
   "source": [
    "### 5.4 Graph+Vector RAG query engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0ee74",
   "metadata": {},
   "source": [
    "This is a combined Graph+Vector Based RAG, where we will retrieve both VectorDB and KG SubGraphs as the context, for synthesis of the answer.\n",
    "\n",
    "```\n",
    "           Graph + Vector RAG with Llama Index\n",
    "                  ┌────┬────┬────┬────┐                  \n",
    "                  │ 1  │ 2  │ 3  │ 4  │                  \n",
    "                  ├────┴────┴────┴────┤                  \n",
    "                  │  Docs/Knowledge   │                  \n",
    "┌───────┐         │        ...        │       ┌─────────┐\n",
    "│       │         ├────┬────┬────┬────┤       │         │\n",
    "│       │         │ 95 │ 96 │    │    │       │         │\n",
    "│       │         └────┴────┴────┴────┘       │         │\n",
    "│ User  │─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─▶   LLM   │\n",
    "│       │                                     │         │\n",
    "│       │                                     │         │\n",
    "└───────┘    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐  └─────────┘\n",
    "    │          ┌──────────────────────────┐        ▲     \n",
    "    └────────┼▶│  Tell me ....., please   │├───────┘     \n",
    "               └──────────────────────────┘              \n",
    "             │ ┌────┐┌────┐               │             \n",
    "               │ 3  ││ 96 │ x->y<-z,x->h...                            \n",
    "             │ └────┘└────┘               │             \n",
    "              ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ \n",
    "```\n",
    "\n",
    "To implement that in Llama Index, we create a `CustomRetriever` to comebine the two: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a9516c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import QueryBundle\n",
    "from llama_index import QueryBundle\n",
    "\n",
    "# import NodeWithScore\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import BaseRetriever, VectorIndexRetriever, KGTableRetriever\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe22b2b",
   "metadata": {},
   "source": [
    "Next, we will create instances of the Vector and KG retrievers, which will be used in the instantiation of the Custom Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecb2d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    ")\n",
    "custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67a63f",
   "metadata": {},
   "source": [
    "And the query engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4976682",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_vector_rag_query_engine = RetrieverQueryEngine(\n",
    "    retriever=custom_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541608be",
   "metadata": {},
   "source": [
    "## 6. Query with all the Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384be7b",
   "metadata": {},
   "source": [
    "### 6.1 Text-to-GraphQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6acaa1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3mGraph Store Query:\n",
      "MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name\n",
      "\u001b[0mERROR:llama_index.graph_stores.nebulagraph:Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name, Param: {}Error message: Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name, Param: {}Error message: SyntaxError: syntax error near `ity`)-[:'\n",
      "Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name, Param: {}Error message: Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name, Param: {}Error message: SyntaxError: syntax error near `ity`)-[:'\n",
      "ERROR:llama_index.graph_stores.nebulagraph:Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name, Param: {}Error message: Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name, Param: {}Error message: SyntaxError: syntax error near `ity`)-[:'\n",
      "Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name, Param: {}Error message: Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name, Param: {}Error message: SyntaxError: syntax error near `ity`)-[:'\n",
      "ERROR:llama_index.graph_stores.nebulagraph:Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name, Param: {}Error message: Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name, Param: {}Error message: SyntaxError: syntax error near `ity`)-[:'\n",
      "Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name, Param: {}Error message: Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\n",
      "WHERE e1.name = 'exit fee'\n",
      "RETURN e2.name, Param: {}Error message: SyntaxError: syntax error near `ity`)-[:'\n"
     ]
    },
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x14187e0f430 state=finished raised ValueError>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\llama_index\\graph_stores\\nebulagraph.py:221\u001b[0m, in \u001b[0;36mNebulaGraphStore.execute\u001b[1;34m(self, query, param_map)\u001b[0m\n\u001b[0;32m    217\u001b[0m     logger\u001b[39m.\u001b[39merror(\n\u001b[0;32m    218\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQuery failed. Query: \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m, Param: \u001b[39m\u001b[39m{\u001b[39;00mparam_map\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError message: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     )\n\u001b[1;32m--> 221\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    222\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    223\u001b[0m     \u001b[39m# other exceptions\u001b[39;00m\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\llama_index\\graph_stores\\nebulagraph.py:197\u001b[0m, in \u001b[0;36mNebulaGraphStore.execute\u001b[1;34m(self, query, param_map)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39mis_succeeded():\n\u001b[1;32m--> 197\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    198\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mQuery failed. Query: \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m, Param: \u001b[39m\u001b[39m{\u001b[39;00mparam_map\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError message: \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m.\u001b[39merror_msg()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mValueError\u001b[0m: Query failed. Query: MATCH (e1:`entity`)-[:relationship]->(e2:`entity`)\nWHERE e1.name = 'exit fee'\nRETURN e2.name, Param: {}Error message: SyntaxError: syntax error near `ity`)-[:'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response_nl2kg \u001b[39m=\u001b[39m nl2kg_query_engine\u001b[39m.\u001b[39;49mquery(\u001b[39m\"\u001b[39;49m\u001b[39mWhat is exit fee?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m display(Markdown(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<b>\u001b[39m\u001b[39m{\u001b[39;00mresponse_nl2kg\u001b[39m}\u001b[39;00m\u001b[39m</b>\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[39m# Cypher:\u001b[39;00m\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\llama_index\\indices\\query\\base.py:23\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m     22\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 23\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n\u001b[0;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\llama_index\\query_engine\\knowledge_graph_query_engine.py:235\u001b[0m, in \u001b[0;36mKnowledgeGraphQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Query the graph store.\"\"\"\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[0;32m    233\u001b[0m     CBEventType\u001b[39m.\u001b[39mQUERY, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str}\n\u001b[0;32m    234\u001b[0m ) \u001b[39mas\u001b[39;00m query_event:\n\u001b[1;32m--> 235\u001b[0m     nodes: List[NodeWithScore] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retrieve(query_bundle)\n\u001b[0;32m    237\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_synthesizer\u001b[39m.\u001b[39msynthesize(\n\u001b[0;32m    238\u001b[0m         query\u001b[39m=\u001b[39mquery_bundle,\n\u001b[0;32m    239\u001b[0m         nodes\u001b[39m=\u001b[39mnodes,\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    242\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose:\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\llama_index\\query_engine\\knowledge_graph_query_engine.py:200\u001b[0m, in \u001b[0;36mKnowledgeGraphQueryEngine._retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    193\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGraph Store Query:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mgraph_store_query\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[0;32m    196\u001b[0m     CBEventType\u001b[39m.\u001b[39mRETRIEVE,\n\u001b[0;32m    197\u001b[0m     payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: graph_store_query},\n\u001b[0;32m    198\u001b[0m ) \u001b[39mas\u001b[39;00m retrieve_event:\n\u001b[0;32m    199\u001b[0m     \u001b[39m# Get the graph store response\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m     graph_store_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph_store\u001b[39m.\u001b[39;49mquery(query\u001b[39m=\u001b[39;49mgraph_store_query)\n\u001b[0;32m    201\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose:\n\u001b[0;32m    202\u001b[0m         print_text(\n\u001b[0;32m    203\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGraph Store Response:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mgraph_store_response\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    204\u001b[0m             color\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myellow\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    205\u001b[0m         )\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\llama_index\\graph_stores\\nebulagraph.py:605\u001b[0m, in \u001b[0;36mNebulaGraphStore.query\u001b[1;34m(self, query, param_map)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquery\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, param_map: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m {}) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 605\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(query, param_map)\n\u001b[0;32m    606\u001b[0m     columns \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mkeys()\n\u001b[0;32m    607\u001b[0m     d: Dict[\u001b[39mstr\u001b[39m, \u001b[39mlist\u001b[39m] \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\tenacity\\__init__.py:326\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreraise:\n\u001b[0;32m    325\u001b[0m         \u001b[39mraise\u001b[39;00m retry_exc\u001b[39m.\u001b[39mreraise()\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m retry_exc \u001b[39mfrom\u001b[39;00m \u001b[39mfut\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait:\n\u001b[0;32m    329\u001b[0m     sleep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait(retry_state)\n",
      "\u001b[1;31mRetryError\u001b[0m: RetryError[<Future at 0x14187e0f430 state=finished raised ValueError>]"
     ]
    }
   ],
   "source": [
    "response_nl2kg = nl2kg_query_engine.query(\"What is exit fee?\")\n",
    "\n",
    "\n",
    "display(Markdown(f\"<b>{response_nl2kg}</b>\"))\n",
    "\n",
    "# Cypher:\n",
    "\n",
    "print(\"Cypher Query:\")\n",
    "\n",
    "graph_query = nl2kg_query_engine.generate_query(\n",
    "    \"What is exit fee?\",\n",
    ")\n",
    "graph_query = graph_query.replace(\"WHERE\", \"\\n  WHERE\").replace(\"RETURN\", \"\\nRETURN\")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "```cypher\n",
    "{graph_query}\n",
    "```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bafef8",
   "metadata": {},
   "source": [
    "### 6.2 Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dec5364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retrievers:> No relationships found, returning nodes found by keywords.\n",
      "> No relationships found, returning nodes found by keywords.\n",
      "INFO:llama_index.indices.knowledge_graph.retrievers:> No nodes found by keywords, returning empty response.\n",
      "> No nodes found by keywords, returning empty response.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "response_graph_rag = kg_rag_query_engine.query(\"Tell me about the borrower.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045c800",
   "metadata": {},
   "source": [
    "### 6.3 Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08fa71a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_rag_query_engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response_vector_rag \u001b[39m=\u001b[39m vector_rag_query_engine\u001b[39m.\u001b[39mquery(\u001b[39m\"\u001b[39m\u001b[39mTell me about Peter Quill.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m display(Markdown(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<b>\u001b[39m\u001b[39m{\u001b[39;00mresponse_vector_rag\u001b[39m}\u001b[39;00m\u001b[39m</b>\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vector_rag_query_engine' is not defined"
     ]
    }
   ],
   "source": [
    "response_vector_rag = vector_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f256d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Result | Graph | Vector |\n",
       "| --- | --- | --- |\n",
       "| Character | Leader of the Guardians of the Galaxy | Half-human, half-Celestial leader of the Guardians of the Galaxy |\n",
       "| Movie | Portrayed by Chris Pratt in the 2014 movie of the same name, and reprised his role in the 2017 sequel. He also wrote and directed the first movie. | In the film, Quill is in a \"state of depression\" following the appearance of a variant of his dead lover Gamora, who does not share the same affection for Quill as her older version had for him, which in turn affects his leadership of the Guardians. |\n",
       "| Personality | Known for his foul language, often using the word \"fuck\". | No mention |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the two QA result on \"Tell me about Peter Quill.\", list the differences between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from Graph: {response_graph_rag}\n",
    "---\n",
    "Result from Vector: {response_vector_rag}\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e1b07",
   "metadata": {},
   "source": [
    "### 6.4 Graph + Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc59b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.knowledge_graph.retriever:> Starting query: Tell me about Peter Quill.\n",
      "> Starting query: Tell me about Peter Quill.\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Query keywords: ['biography', 'Peter Quill', 'history', 'Peter', 'Quill']\n",
      "> Query keywords: ['biography', 'Peter Quill', 'history', 'Peter', 'Quill']\n",
      "INFO:llama_index.indices.knowledge_graph.retriever:> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'released in', '2014']\n",
      "Peter Quill ['portrays', 'Peter Quill']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'reprised role from', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'directed', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'wrote', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'sequel to', 'Guardians of the Galaxy']\n",
      "Quill ['speaks', ' fuck ']\n",
      "> Extracted relationships: The following are knowledge triplets in max depth 2 in the form of `subject [predicate, object, predicate_next_hop, object_next_hop ...]`\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'released in', '2014']\n",
      "Peter Quill ['portrays', 'Peter Quill']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'reprised role from', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'directed', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'wrote', 'Guardians of the Galaxy']\n",
      "Peter Quill ['is leader of', 'Guardians of the Galaxy', 'sequel to', 'Guardians of the Galaxy']\n",
      "Quill ['speaks', ' fuck ']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>\n",
       "Peter Quill is the half-human, half-Celestial leader of the Guardians of the Galaxy, a group of alien thieves and smugglers. He was abducted from Earth as a child and raised by the Ravagers. Quill reprised his role from the 2014 film Guardians of the Galaxy, which he also directed and wrote. He is the sequel to the original Guardians of the Galaxy and is known to speak with a foul mouth.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_graph_vector_rag = graph_vector_rag_query_engine.query(\"Tell me about Peter Quill.\")\n",
    "\n",
    "display(Markdown(f\"<b>{response_graph_vector_rag}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f82da8",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697e4bc",
   "metadata": {},
   "source": [
    "### 7.1 Overall Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cbae28",
   "metadata": {},
   "source": [
    "Let's compare the results of them.\n",
    "\n",
    "First check the information that were coverred by different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed1e85e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Knowledge Fact | text2GraphQuery | Graph | Vector | Graph+Vector |\n",
       "| -------------- | --------------- | ----- | ------ | ----------- |\n",
       "| Member of Guardians of the Galaxy | Yes | Yes | No | Yes |\n",
       "| Leader of Guardians of the Galaxy | No | Yes | No | Yes |\n",
       "| Portrayed by Chris Pratt | No | Yes | No | Yes |\n",
       "| Wrote and directed first movie | No | Yes | No | Yes |\n",
       "| Foul language | No | Yes | No | Yes |\n",
       "| Half-human, half-Celestial | No | No | Yes | Yes |\n",
       "| Abducted from Earth as a child | No | No | Yes | Yes |\n",
       "| Raised by a group of alien thieves and smugglers | No | No | Yes | Yes |\n",
       "| In a \"state of depression\" | No | No | Yes | No |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the QA results on \"Tell me about Peter Quill.\", list the knowledge facts between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result text2GraphQuery: {response_nl2kg}\n",
    "---\n",
    "Result Graph: {response_graph_rag}\n",
    "---\n",
    "Result Vector: {response_vector_rag}\n",
    "---\n",
    "Result Graph+Vector: {response_graph_vector_rag}\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377a880",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- The pure **KG**(both text2GraphQuery and Graph RAG) comes with **concise** results, and much **lower cost**(for cost comparision see our previous result [here](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html#comparison-of-results) )\n",
    "- The **Graph+Vector** RAG could be more **comprehensive** in case the question envolves knowledge that's fine-grained **spread** across more chunks than top-K searching.\n",
    "\n",
    "\n",
    "| QueryEngine | Knowledge Graph query engine                                 | Graph RAG query engine                                       | Vector RAG query engine                                      | Graph Vector RAG query engine                                |\n",
    "| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| Mechanism   | 1. **Text-to-GraphQuery** based on KG<br />2. Query KG with the result<br />3. Answer synthesis based on query result | 1. Get related entities of the question<br />2. Get n-depth **SubGraphs** of related entities from KG<br />3. Answer synthesis based on related SubGraphs | 1. Create embedding of question<br />2. Semantic search **top-k related doc chunks**<br />3. Answer synthesis based on related doc chunks | 1. Do retrieval as Vector and Graph RAG <br />2. Answer synthesis based on **both related chunks and SubGraphs** |\n",
    "| Performance | Concise                                                      | Concise                                                      | Fruitful                                                     | Fruitful, could be more comprehensive                        |\n",
    "| Cost        | Low                                                          | Low                                                          | High                                                         | High                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae85ed",
   "metadata": {},
   "source": [
    "### 7.2 Text2GraphQuery vs Graph RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48a35d",
   "metadata": {},
   "source": [
    "In **Text2GraphQuery**, we leverage the LLM to compose a Graph Query that's trying to provide the answer in the `RETURN` fields. While, on the other hands, the **Graph RAG** find all related knowledges as the context.\n",
    "\n",
    "So when the answer by nature refers to small piece of information as answer, Text2GraphQuery could do the job in the best, and in other cases, the Graph RAG could be better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4510b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Feature | Text-to-Graph | Graph RAG |\n",
       "| --- | --- | --- |\n",
       "| Superhero Team | Guardians of the Galaxy | Guardians of the Galaxy |\n",
       "| Movie | N/A | 2014 and 2017 |\n",
       "| Role | N/A | Portrayed by Chris Pratt |\n",
       "| Writing/Directing | N/A | Wrote and Directed the first movie |\n",
       "| Character Trait | N/A | Foul language, often using the word \"fuck\" |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        llm(f\"\"\"\n",
    "Compare the two QA result on \"Tell me about Peter Quill.\", list the differences between them, to help evalute them. Output in markdown table.\n",
    "\n",
    "Result from text-to-Graph: {response_nl2kg}\n",
    "---\n",
    "Result from Graph RAG: {response_graph_rag}\n",
    "\n",
    "\"\"\"\n",
    "           )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33632e85",
   "metadata": {},
   "source": [
    "We could visulize the knowledge context that were coverred during the two approach, we could see their difference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a01d88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_text2cypher = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53e78497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nebula3.logger:Get connection to ('127.0.0.1', 9669)\n",
      "Get connection to ('127.0.0.1', 9669)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})&lt;-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              path_0\n",
       "0  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...\n",
       "1  (\"Peter Quill\" :entity{name: \"Peter Quill\"})<-...\n",
       "2  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...\n",
       "3  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...\n",
       "4  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[...\n",
       "5  (\"Peter Quill\" :entity{name: \"Peter Quill\"})-[..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ngql\n",
    "MATCH path_0=(p:`entity`)-[*1..2]-()\n",
    "  WHERE p.`entity`.`name` == 'Peter Quill' \n",
    "RETURN path_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d05dbbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67f8f597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "    <tr>\n",
       "        <th>Text2Cypher Traversed knowledge</th>\n",
       "        <th>Graph Rag Traversed knowledge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>\n",
       "<iframe\n",
       "    src=\"https://www.siwei.io/demo-dumps/kg-llm/nebulagraph_draw_nl2cypher.html\"\n",
       "    width=450\n",
       "    height=400>\n",
       "</iframe>\n",
       "</td>\n",
       "        <td>\n",
       "<iframe\n",
       "    src=\"https://www.siwei.io/demo-dumps/kg-llm/nebulagraph_draw_rag.html\"\n",
       "    width=450\n",
       "    height=400>\n",
       "</iframe>\n",
       "</td>\n",
       "    </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text2Cypher = \"\"\"\n",
    "<iframe\n",
    "    src=\"https://www.siwei.io/demo-dumps/kg-llm/nebulagraph_draw_nl2cypher.html\"\n",
    "    width=450\n",
    "    height=400>\n",
    "</iframe>\n",
    "\"\"\"\n",
    "graphRAG = \"\"\"\n",
    "<iframe\n",
    "    src=\"https://www.siwei.io/demo-dumps/kg-llm/nebulagraph_draw_rag.html\"\n",
    "    width=450\n",
    "    height=400>\n",
    "</iframe>\n",
    "\"\"\"\n",
    "\n",
    "table = f\"\"\"\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Text2Cypher Traversed knowledge</th>\n",
    "        <th>Graph Rag Traversed knowledge</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>{text2Cypher}</td>\n",
    "        <td>{graphRAG}</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be0b42",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "For those tasks:\n",
    "\n",
    "- Potentially cares more relationed knowledge\n",
    "- Schema of the KG is sophisticated to be hard for text2cypher to express the task\n",
    "- KG quality isn't good enough\n",
    "- Multiple \"starting entities\" are involved\n",
    "\n",
    "Graph RAG could be a better approach to start with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
