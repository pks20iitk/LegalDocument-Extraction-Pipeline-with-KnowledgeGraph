{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0mr4EqDHw1U"
      },
      "source": [
        "# This notebook is made to test out Claude from Anthropic.\n",
        "### The code below leverages AWS Bedrock services to access Claude.\n",
        "\n",
        "\n",
        "\n",
        "*   Make sure you use ENV variables instead of hardcoding\n",
        "*   Do not commit/push this notebook with env variables\n",
        "*   Have Fun!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsV9AOsukSMC"
      },
      "source": [
        "#### Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS6SDVgJRQHR"
      },
      "outputs": [],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4uhSMkykaBj"
      },
      "source": [
        "#### Importing packages and environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-so9uxocRRGT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "import json\n",
        "import base64\n",
        "\n",
        "# Set an environment variable here\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = 'Aws access key id'\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = 'Aws secret access key'\n",
        "os.environ['REGION_NAME'] = 'Region from aws account, find it through IAM'\n",
        "os.environ['AWS_RUNTIME'] = 'Service runtime'\n",
        "\n",
        "# Saving env values in variables\n",
        "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
        "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
        "REGION_NAME = os.getenv('REGION_NAME')\n",
        "AWS_RUNTIME = os.getenv('AWS_RUNTIME')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj1kOIKBlg0K"
      },
      "source": [
        "#### Initializing boto3 client using runtime and aws keys\n",
        "\n",
        "\n",
        "*   `AWS_RUNTIME`: Determines the service we want to use.\n",
        "*   `AWS_ACCESS_KEY_ID`: Account access key Id from AWS. (AWS console > IAM > Dashboard > \"Manage Access Keys\")\n",
        "*   `AWS_SECRET_ACCESS_KEY`: Account seceret access key. (Get from IAM)\n",
        "*   `REGION_NAME`: Default configured region name from AWS account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "e0ZWfqPmRVEr"
      },
      "outputs": [],
      "source": [
        "client = boto3.client(\n",
        "    AWS_RUNTIME,\n",
        "    aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key = AWS_SECRET_ACCESS_KEY,\n",
        "    region_name= REGION_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdDPlxnemyE-"
      },
      "source": [
        "#### Creating variable `input_data` to pass into the `invoke_client()` method from boto3 client initialized in the previous step.\n",
        "\n",
        "Inside `input_data` we can specify all the model parameters and prompt:\n",
        "\n",
        "\n",
        "*   `modelId`: Defunes which model to use from bedrock\n",
        "*   `contentType`: Defines the response type, default type is `JSON`.\n",
        "*   `accept`: Defines the input type, default type is `String`.\n",
        "*   `body`: Body objects defines multiple parameters:\n",
        "\n",
        "    1.   **prompt**: Takes in the user prompt.\n",
        "    2.   **max_tokens_to_sample**: Defines the maximum output tokens.\n",
        "    3.   **temperature**: Defines the temperature for the model output.\n",
        "    4.   **top_k**: Sample from the k most likely next tokens at each step. Lower k focuses on higher probability tokens.\n",
        "    5.   **top_p**: The cumulative probability cutoff for token selection. Lower values mean sampling from a smaller, more top-weighted nucleus.\n",
        "    6.   **anthropic_version**: Version of the deployed model on bedrock.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "l33QJa3xguaj"
      },
      "outputs": [],
      "source": [
        "# Specify the input data and model ID\n",
        "input_data = {\n",
        "  \"modelId\": \"anthropic.claude-v2\",\n",
        "  \"contentType\": \"application/json\",\n",
        "  \"accept\": \"*/*\",\n",
        "  \"body\": {\n",
        "    \"prompt\": \"\\n\\nHuman: write poem to make me happy!\\n\\nAssistant:\",\n",
        "    \"max_tokens_to_sample\": 300,\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_k\": 250,\n",
        "    \"top_p\": 1,\n",
        "    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBCc__wzq4iG"
      },
      "source": [
        "#### Using the boto3 client initialized earlier to invoke the deployed models using the `invoke_model()` methon.\n",
        "\n",
        "* The below code uses `invoke_model_with_response_stream()` method to recive data in chunks, which means we can yield these chunks in order to stream responses from the llm.\n",
        "\n",
        "* You can also use `invoke_model()` and pass in the same parameters to get the full response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdYd6o8tg0YM"
      },
      "outputs": [],
      "source": [
        "# Invoke the model for inference\n",
        "response = client.invoke_model_with_response_stream(contentType=input_data[\"contentType\"], body=json.dumps(input_data['body']), modelId=input_data['modelId'])\n",
        "event_stream = response['body']\n",
        "\n",
        "# Iterate through events in the stream\n",
        "for event in event_stream:\n",
        "    # Event messages might be bytes, requiring decoding, and potentially deserialization from JSON\n",
        "    print(event['chunk'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
