{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_base = os.getenv(\"OPENAI_BASE\")\n",
    "openai_deployment = os.getenv(\"AZURE_OPEN_AI_MODEL\")\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://prod-open-ai-service.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"30c82b416ff4456faa26b1644a83ab4b\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data-legal\"\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(DATA_PATH).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.llms import LangChainLLM\n",
    "from llama_index import LangchainEmbedding\n",
    "\n",
    "# load documents\n",
    "\n",
    "\n",
    "# initialize service context (set chunk size)\n",
    "# -- here, we set a smaller chunk size, to allow for more effective re-ranking\n",
    "llm = AzureOpenAI(\n",
    "    engine=\"PROD-GPT-16K-TURBO\",\n",
    "    temperature=0, openai_api_type=\"azure\",\n",
    "    openai_api_key=openai_key,\n",
    "    openai_api_base=openai_base,\n",
    "    openai_api_version=\"2023-05-15\")\n",
    "\n",
    "embedding_llm = LangchainEmbedding(\n",
    "    OpenAIEmbeddings(\n",
    "        deployment=\"PROD-text-embedding-ada-002\",\n",
    "        openai_api_type=\"azure\",\n",
    "        openai_api_key=openai_key,\n",
    "        openai_api_base=openai_base,\n",
    "        openai_api_version=\"2023-05-15\",\n",
    "        chunk_size=1500,\n",
    "    ),\n",
    "    embed_batch_size=16,\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embedding_llm,\n",
    ")\n",
    "# service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import BM25Retriever\n",
    "\n",
    "# retireve the top 10 most similar nodes using embeddings\n",
    "vector_retriever = index.as_retriever(similarity_top_k=30)\n",
    "\n",
    "# retireve the top 10 most similar nodes using bm25\n",
    "bm25_retriever = BM25Retriever.from_defaults(index, similarity_top_k=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import BaseRetriever\n",
    "\n",
    "\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, bm25_retriever):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "        vector_nodes = self.vector_retriever.retrieve(query, **kwargs)\n",
    "\n",
    "        # combine the two lists of nodes\n",
    "        all_nodes = []\n",
    "        node_ids = set()\n",
    "        for n in bm25_nodes + vector_nodes:\n",
    "            if n.node.node_id not in node_ids:\n",
    "                all_nodes.append(n)\n",
    "                node_ids.add(n.node.node_id)\n",
    "        return all_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.as_retriever(similarity_top_k=30)\n",
    "\n",
    "hybrid_retriever = HybridRetriever(vector_retriever, bm25_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "reranker = SentenceTransformerRerank(\n",
    "    top_n=30, model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import LLMRerank\n",
    "\n",
    "reranker = LLMRerank(service_context=service_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquery_engine\u001b[39;00m \u001b[39mimport\u001b[39;00m RetrieverQueryEngine\n\u001b[0;32m      3\u001b[0m query_engine \u001b[39m=\u001b[39m RetrieverQueryEngine\u001b[39m.\u001b[39mfrom_args(\n\u001b[0;32m      4\u001b[0m     retriever\u001b[39m=\u001b[39mhybrid_retriever,\n\u001b[0;32m      5\u001b[0m     node_postprocessors\u001b[39m=\u001b[39m[reranker],\n\u001b[0;32m      6\u001b[0m     service_context\u001b[39m=\u001b[39mservice_context,\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m response \u001b[39m=\u001b[39m query_engine\u001b[39m.\u001b[39;49mquery(\u001b[39m\"\u001b[39;49m\u001b[39mWhat is the name of the borrower ?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\llama_index\\indices\\query\\base.py:23\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m     22\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 23\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n\u001b[0;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\llama_index\\query_engine\\retriever_query_engine.py:165\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[0;32m    159\u001b[0m     CBEventType\u001b[39m.\u001b[39mQUERY, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str}\n\u001b[0;32m    160\u001b[0m ) \u001b[39mas\u001b[39;00m query_event:\n\u001b[0;32m    161\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[0;32m    162\u001b[0m         CBEventType\u001b[39m.\u001b[39mRETRIEVE,\n\u001b[0;32m    163\u001b[0m         payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str},\n\u001b[0;32m    164\u001b[0m     ) \u001b[39mas\u001b[39;00m retrieve_event:\n\u001b[1;32m--> 165\u001b[0m         nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve(query_bundle)\n\u001b[0;32m    167\u001b[0m         retrieve_event\u001b[39m.\u001b[39mon_end(\n\u001b[0;32m    168\u001b[0m             payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mNODES: nodes},\n\u001b[0;32m    169\u001b[0m         )\n\u001b[0;32m    171\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_synthesizer\u001b[39m.\u001b[39msynthesize(\n\u001b[0;32m    172\u001b[0m         query\u001b[39m=\u001b[39mquery_bundle,\n\u001b[0;32m    173\u001b[0m         nodes\u001b[39m=\u001b[39mnodes,\n\u001b[0;32m    174\u001b[0m     )\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\llama_index\\query_engine\\retriever_query_engine.py:114\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mretrieve\u001b[39m(\u001b[39mself\u001b[39m, query_bundle: QueryBundle) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m    113\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retriever\u001b[39m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m--> 114\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_node_postprocessors(nodes, query_bundle\u001b[39m=\u001b[39;49mquery_bundle)\n\u001b[0;32m    116\u001b[0m     \u001b[39mreturn\u001b[39;00m nodes\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\llama_index\\query_engine\\retriever_query_engine.py:107\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._apply_node_postprocessors\u001b[1;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply_node_postprocessors\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m, nodes: List[NodeWithScore], query_bundle: QueryBundle\n\u001b[0;32m    105\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m    106\u001b[0m     \u001b[39mfor\u001b[39;00m node_postprocessor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_node_postprocessors:\n\u001b[1;32m--> 107\u001b[0m         nodes \u001b[39m=\u001b[39m node_postprocessor\u001b[39m.\u001b[39;49mpostprocess_nodes(\n\u001b[0;32m    108\u001b[0m             nodes, query_bundle\u001b[39m=\u001b[39;49mquery_bundle\n\u001b[0;32m    109\u001b[0m         )\n\u001b[0;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m nodes\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\llama_index\\indices\\postprocessor\\llm_rerank.py:63\u001b[0m, in \u001b[0;36mLLMRerank.postprocess_nodes\u001b[1;34m(self, nodes, query_bundle)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39m# call each batch independently\u001b[39;00m\n\u001b[0;32m     57\u001b[0m raw_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context\u001b[39m.\u001b[39mllm_predictor\u001b[39m.\u001b[39mpredict(\n\u001b[0;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_choice_select_prompt,\n\u001b[0;32m     59\u001b[0m     context_str\u001b[39m=\u001b[39mfmt_batch_str,\n\u001b[0;32m     60\u001b[0m     query_str\u001b[39m=\u001b[39mquery_str,\n\u001b[0;32m     61\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m raw_choices, relevances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_choice_select_answer_fn(\n\u001b[0;32m     64\u001b[0m     raw_response, \u001b[39mlen\u001b[39;49m(nodes_batch)\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     66\u001b[0m choice_idxs \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(choice) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m choice \u001b[39min\u001b[39;00m raw_choices]\n\u001b[0;32m     67\u001b[0m choice_nodes \u001b[39m=\u001b[39m [nodes_batch[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m choice_idxs]\n",
      "File \u001b[1;32md:\\Pragyaa\\.venv\\lib\\site-packages\\llama_index\\indices\\utils.py:103\u001b[0m, in \u001b[0;36mdefault_parse_choice_select_answer_fn\u001b[1;34m(answer, num_choices, raise_error)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     99\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid answer line: \u001b[39m\u001b[39m{\u001b[39;00manswer_line\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAnswer line must be of the form: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39manswer_num: <int>, answer_relevance: <float>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m         )\n\u001b[1;32m--> 103\u001b[0m answer_num \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(line_tokens[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m:\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mstrip())\n\u001b[0;32m    104\u001b[0m \u001b[39mif\u001b[39;00m answer_num \u001b[39m>\u001b[39m num_choices:\n\u001b[0;32m    105\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=hybrid_retriever,\n",
    "    node_postprocessors=[reranker],\n",
    "    service_context=service_context,\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What is the name of the borrower ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** The name of the borrower in the loan agreement is Borrower."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
